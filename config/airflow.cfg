[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow DAGs are stored
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# Whether to load the examples that ship with Airflow
load_examples = False

# Default timezone in case supplied date times are naive
default_timezone = UTC

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to pickle DAG object and send over to DagFileProcessor
store_dag_code = True

[database]
# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = sqlite:////opt/airflow/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Secret key used to run your flask app
secret_key = stockelper-secret-key-change-in-production

# Number of workers to run the Gunicorn web server
workers = 4

# The worker timeout for gunicorn
worker_timeout = 120

# Expose the configuration file in the web server
expose_config = True

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# Default DAG orientation. Valid values are:
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@stockelper.com

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks kill`),
# this defines the frequency at which they should listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# after how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
scheduler_health_check_threshold = 30

# Number of seconds to wait before the scheduler purges old task logs
# Set to 0 to disable automatic log cleanup
schedule_after_task_execution = 0

# Whether to enable log cleanup
enable_log_cleanup = True

# How often to run the log cleanup job (in seconds)
# Default is 86400 (24 hours)
log_cleanup_interval = 86400

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Users must supply an Airflow connection id that provides access to the storage
# location. If remote_logging is set to true, see UPDATING.md for additional
# configuration requirements.
remote_logging = False

# Logging level
logging_level = INFO

# Log retention settings
# Number of days to retain task instance logs (default: 30)
# Set to 0 to disable automatic log cleanup
log_retention_days = 7

# Maximum number of log files to keep per task instance
# Older logs will be deleted automatically
max_log_files_per_task = 5

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
logging_config_class = 

# Log format for when Airflow writes logs to a file.
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

[metrics]
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[celery]
# This section only applies if you are using the CeleryExecutor in
# [core] section above

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# "airflow celery worker" command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
broker_url = redis://redis:6379/0

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
result_backend = redis://redis:6379/0

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it `airflow celery flower`. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
flower_url_prefix = 

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower is the responsibility of the user. However, you can set a
# password for Flower using the password option in the [celery] section.
flower_basic_auth = 

[dask]
# This section only applies if you are using the DaskExecutor in
# [core] section above

# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca = 
tls_cert = 
tls_key = 

[kubernetes]
# This section only applies if you are using the KubernetesExecutor in
# [core] section above

# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository = 
worker_container_tag = 
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# The Kubernetes namespace where airflow workers should be created. Defaults to `default`
namespace = default

# The Value of the field path "metadata.labels['airflow_version']" on the worker POD.
airflow_configmap = 

# For either git sync or volume mounted data, the worker container will use this image
dags_volume_claim = 

# For DAGs mounted via a volume claim (mutually exclusive with volume claim)
dags_volume_host = 

# For volume mounted logs, the worker container will use this image
logs_volume_claim = 

# For volume mounted logs, the worker container will use this image
logs_volume_host = 

# Amount of time (in seconds) to wait before timing out a python kubernetes client request
kube_client_request_args = 

# Optional keyword arguments to pass to the `delete_namespaced_pod` kubernetes client
# `delete_option` method when deleting worker pods.
delete_option_kwargs = 

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
worker_pods_creation_batch_size = 1

# The scheduler runs multiple threads in parallel to schedule dags. This defines
# how many threads will run.
max_threads = 2
